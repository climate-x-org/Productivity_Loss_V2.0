{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import pandas as pd\n",
    "import numpy as np  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "import s3fs, dask\n",
    "fs = s3fs.S3FileSystem(anon=False, use_listings_cache=False, asynchronous=True)\n",
    "dask.config.set(\n",
    "    {\"s3.multipart_chunksize\": \"50MB\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(fn,encoding='unicode_escape')[['Asset ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../data/siemens-energy-ag.csv'\n",
    "loss_function = 'HOTHAPS'\n",
    "\n",
    "\n",
    "## read csv  \n",
    "df_in = pd.read_csv(fn,encoding='unicode_escape')[['Asset ID','Latitude','Longitude','Parent Name','Premise Type','Country']]\n",
    "df_in = df_in.drop_duplicates('Asset ID').rename(columns={'Latitude':'lat','Longitude':'lon','Country':'country'})\n",
    "## load asset map \n",
    "asset_map = pd.read_csv('../src/asset_map.csv')\n",
    "\n",
    "## load aircon\n",
    "aircon = xr.open_zarr('s3://hazard-science-data/productivity_loss_v2/aircon/AirCon_SSPs.zarr/') \n",
    "aircon = aircon.sel(SSP='2.0').isel(year=0)['ac_penetration'].chunk(dict(lon=-1)).interpolate_na(method='nearest',dim='lon',limit=3)\n",
    "\n",
    "\n",
    "## load losses\n",
    "ds_dict = {}\n",
    "ds_2020 = {}\n",
    "for intensity in ['low','moderate','high']:\n",
    "    ds_2020[intensity] = xr.open_zarr(f's3://hazard-science-data/productivity_loss_v2/climate_outputs/observations/ERA5_{loss_function}_productivity_loss_{intensity}.zarr.zarr/')\n",
    "for scenario in ['ssp126','ssp245','ssp370','ssp585']:\n",
    "    ds_dict[scenario] = {}\n",
    "    for intensity in ['low','moderate','high']:\n",
    "        ds_dict[scenario][intensity] = xr.open_zarr(f\"s3://hazard-science-data/productivity_loss_v2/climate_outputs/projections_corrected/{scenario}/CMIP6-ScenarioMIP_{loss_function}_productivity_loss_{intensity}_{scenario}.zarr.zarr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scenarios = ['ssp585']\n",
    "\n",
    "# List to accumulate all rows (one per asset_id, year, scenario)\n",
    "rows_list = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Create a temporary copy of your asset data and map work_intensity\n",
    "    df_temp = df_in.loc[:40, :].copy(deep=True)\n",
    "    df_temp['work_intensity'] = df_temp['asset_type'].map(asset_map.set_index('asset_type')['intensity'])\n",
    "    \n",
    "    # Get the years available for this scenario.\n",
    "    # Here, we assume the years are the same for every asset in a scenario.\n",
    "    years = np.concat([[2020],ds_dict[scenario]['high'].year.values])\n",
    "    stats = ['median', 'minimum', 'maximum']\n",
    "    \n",
    "    # Process each asset row\n",
    "    for _, row in df_temp.iterrows():\n",
    "        # Get the dataset corresponding to the asset's work intensity in the current scenario\n",
    "        ds = ds_dict[scenario][row['work_intensity']]\n",
    "        ds_2020_local = ds_2020[row['work_intensity']].sel(lat=row['lat'], lon=row['lon'], method='nearest')\n",
    "\n",
    "        # For each year, extract the values for all stats and build a new row\n",
    "        for year in years:\n",
    "            row_dict = row.to_dict()  # Copy the asset info (like asset_id, lat, lon, etc.)\n",
    "            row_dict['scenario'] = scenario\n",
    "            \n",
    "            if year == 2020:\n",
    "                row_dict['year'] = 2020\n",
    "                for stat in stats:\n",
    "                    value = np.round(ds_2020_local['median'].values,2)\n",
    "                    row_dict[stat] = value\n",
    "\n",
    "            else:\n",
    "                row_dict['year'] = year\n",
    "                # Get each statistic value from the dataset (using nearest neighbor)\n",
    "                for stat in stats:\n",
    "                    value = np.round(ds.sel(lat=row['lat'], lon=row['lon'], year=year, method='nearest')[stat].values, 2)\n",
    "                    row_dict[stat] = value\n",
    "                # Append the new row to our list\n",
    "            rows_list.append(row_dict)\n",
    "\n",
    "# Create the final long DataFrame\n",
    "df_long = pd.DataFrame(rows_list)\n",
    "df_long.to_csv(f\"../output_csvs/unscaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a deep copy of df_long so that the original remains unchanged\n",
    "df_long_scaled = df_long.copy(deep=True)\n",
    "\n",
    "# Create a new column for AC_penetration and initialize with NaN\n",
    "df_long_scaled['AC_penetration'] = np.nan\n",
    "\n",
    "# Iterate over each row to compute AC penetration and adjust the values\n",
    "for i in df_long_scaled.index:\n",
    "    lat = df_long_scaled.loc[i, 'lat']\n",
    "    lon = df_long_scaled.loc[i, 'lon']\n",
    "    # Compute AC_penetration using the nearest neighbor from the aircon dataset\n",
    "    ac_val = np.round(aircon.sel(lat=lat, lon=lon, method='nearest').values, 2)\n",
    "    df_long_scaled.loc[i, 'AC_penetration'] = ac_val\n",
    "\n",
    "    # If a valid AC penetration value is found, scale the statistics accordingly\n",
    "    if not pd.isna(ac_val):\n",
    "        # Scale 'median', 'minimum', and 'maximum' by multiplying with (1 - AC_penetration)\n",
    "        df_long_scaled.loc[i, ['median', 'minimum', 'maximum']] *= (1 - ac_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define scenarios, e.g., ['ssp126', 'ssp585']\n",
    "scenarios = ['ssp126','ssp585']\n",
    "\n",
    "# Select 25 random asset_ids from the entire long-format DataFrame\n",
    "sample_ids = random.sample(list(df_long_scaled['asset_id'].unique()), 25)\n",
    "\n",
    "# Create a subplot with one row per scenario\n",
    "f, ax = plt.subplots(len(scenarios), 1, figsize=(10, 3 * len(scenarios)), sharex=True)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    # Filter for the current scenario, the sampled asset_ids, and only the years 2025 and 2100.\n",
    "    data = df_long_scaled[\n",
    "        (df_long_scaled['scenario'] == scenario) &\n",
    "        (df_long_scaled['asset_id'].isin(sample_ids)) &\n",
    "        (df_long_scaled['year'].isin([2025, 2100]))\n",
    "    ].copy()\n",
    "    \n",
    "    # Pivot so each asset_id is one row, and columns become \"2025_median\", \"2025_minimum\", etc.\n",
    "    data_pivot = data.pivot(index='asset_id', columns='year', values=['median', 'minimum', 'maximum'])\n",
    "    # Flatten the MultiIndex columns to single strings.\n",
    "    data_pivot.columns = [f\"{year}_{stat}\" for stat, year in data_pivot.columns]\n",
    "    data_pivot = data_pivot.reset_index()\n",
    "\n",
    "    # Define x-axis positions for each asset.\n",
    "    x = np.arange(len(data_pivot))\n",
    "\n",
    "    # Define error bars for 2025 and 2100.\n",
    "    err_2025 = [np.abs(data_pivot[\"2025_median\"] - data_pivot[\"2025_minimum\"]),\n",
    "                np.abs(data_pivot[\"2025_maximum\"] - data_pivot[\"2025_median\"])]\n",
    "    err_2100 = [np.abs(data_pivot[\"2100_median\"] - data_pivot[\"2100_minimum\"]),\n",
    "                np.abs(data_pivot[\"2100_maximum\"] - data_pivot[\"2100_median\"])]\n",
    "\n",
    "    bar_width = 0.4\n",
    "    # Select the appropriate axis handle.\n",
    "    a = ax[i] if len(scenarios) > 1 else ax\n",
    "\n",
    "    # Plot bars with error bars for both years.\n",
    "    a.bar(x - bar_width / 2, data_pivot[\"2025_median\"], yerr=err_2025, width=bar_width,\n",
    "          capsize=5, label=\"2020\", color=\"xkcd:navy\")\n",
    "    a.bar(x + bar_width / 2, data_pivot[\"2100_median\"], yerr=err_2100, width=bar_width,\n",
    "          capsize=5, label=\"2100\", color=\"xkcd:ruby\")\n",
    "\n",
    "    a.set_xticks(x)\n",
    "    a.set_xticklabels(data_pivot['asset_id'], rotation=45,fontsize=6,ha = 'right')\n",
    "    a.set_ylim(0, 50)\n",
    "    # a.set_yscale('log')\n",
    "    a.set_ylabel('Productivity Loss (%)')\n",
    "    a.legend()\n",
    "    a.set_title(scenario)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define scenarios, e.g., ['ssp126', 'ssp585']\n",
    "scenarios = ['ssp126','ssp585']\n",
    "\n",
    "# Select 25 random asset_ids from the entire long-format DataFrame\n",
    "sample_ids = random.sample(list(df_long_scaled['asset_id'].unique()), 25)\n",
    "\n",
    "# Create a subplot with one row per scenario\n",
    "f, ax = plt.subplots(len(scenarios), 1, figsize=(10, 3 * len(scenarios)), sharex=True)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    # Filter for the current scenario, the sampled asset_ids, and only the years 2025 and 2100.\n",
    "    data = df_long_scaled[\n",
    "        (df_long_scaled['scenario'] == scenario) &\n",
    "        (df_long_scaled['asset_id'].isin(sample_ids)) &\n",
    "        (df_long_scaled['year'].isin([2025, 2100]))\n",
    "    ].copy()\n",
    "    \n",
    "    # Pivot so each asset_id is one row, and columns become \"2025_median\", \"2025_minimum\", etc.\n",
    "    data_pivot = data.pivot(index='asset_id', columns='year', values=['median', 'minimum', 'maximum'])\n",
    "    # Flatten the MultiIndex columns to single strings.\n",
    "    data_pivot.columns = [f\"{year}_{stat}\" for stat, year in data_pivot.columns]\n",
    "    data_pivot = data_pivot.reset_index()\n",
    "\n",
    "    # Define x-axis positions for each asset.\n",
    "    x = np.arange(len(data_pivot))\n",
    "\n",
    "    # Define error bars for 2025 and 2100.\n",
    "    err_2025 = [np.abs(data_pivot[\"2025_median\"] - data_pivot[\"2025_minimum\"]),\n",
    "                np.abs(data_pivot[\"2025_maximum\"] - data_pivot[\"2025_median\"])]\n",
    "    err_2100 = [np.abs(data_pivot[\"2100_median\"] - data_pivot[\"2100_minimum\"]),\n",
    "                np.abs(data_pivot[\"2100_maximum\"] - data_pivot[\"2100_median\"])]\n",
    "\n",
    "    bar_width = 0.4\n",
    "    # Select the appropriate axis handle.\n",
    "    a = ax[i] if len(scenarios) > 1 else ax\n",
    "\n",
    "    # Plot bars with error bars for both years.\n",
    "    a.bar(x - bar_width / 2, data_pivot[\"2025_median\"], yerr=err_2025, width=bar_width,\n",
    "          capsize=5, label=\"2020\", color=\"xkcd:grey\")\n",
    "    a.bar(x + bar_width / 2, data_pivot[\"2100_median\"], yerr=err_2100, width=bar_width,\n",
    "          capsize=5, label=\"2100\", color=\"xkcd:black\")\n",
    "\n",
    "    a.set_xticks(x)\n",
    "    a.set_xticklabels(data_pivot['asset_id'], rotation=45,fontsize=6,ha = 'right')\n",
    "    a.set_ylim(0.01, 100)\n",
    "    a.set_yscale('log')\n",
    "    a.set_ylabel('Productivity Loss (%)')\n",
    "    a.legend()\n",
    "    a.set_title(scenario)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = random.sample(list(df_long_scaled['asset_id'].unique()),10)\n",
    "f,ax = plt.subplots(len(ids),1,figsize=(7,2*len(ids)),sharex=True)\n",
    "\n",
    "colors = {\n",
    "    'ssp126':'xkcd:navy',\n",
    "    'ssp245':'xkcd:teal',\n",
    "    'ssp370':'xkcd:orange',\n",
    "    'ssp585':'xkcd:ruby'\n",
    "}\n",
    "\n",
    "for j,id in enumerate(ids):\n",
    "    if len(scenarios)>1:\n",
    "        a = ax[j]\n",
    "    else:\n",
    "        a = ax\n",
    "    \n",
    "\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        y = df_long_scaled[(df_long_scaled['asset_id']==id)&(df_long_scaled['scenario']==scenario)]['median']\n",
    "        yhigh = df_long_scaled[(df_long_scaled['asset_id']==id)&(df_long_scaled['scenario']==scenario)]['maximum']\n",
    "        ylow = df_long_scaled[(df_long_scaled['asset_id']==id)&(df_long_scaled['scenario']==scenario)]['minimum']\n",
    "        x = [2020,2025,2030,2035,2040,2045,2050,2060,2070,2080,2090,2100]\n",
    "\n",
    "        a.fill_between(x,ylow,yhigh,alpha=0.2,color=colors[scenario])\n",
    "        a.plot(x,y,color=colors[scenario],label=scenario)\n",
    "    a.set_ylim(0,np.max(yhigh)+1)\n",
    "    plt.xlim(2020,2100)\n",
    "    # a.set_yscale('log')\n",
    "    a.set_ylabel('Productivity Loss (%)')\n",
    "    a.legend()\n",
    "    a.set_title(id + ' (' + df_long_scaled[(df_long_scaled['asset_id']==id)]['country'].unique() + ')',fontsize=9)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "productivity_loss_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
