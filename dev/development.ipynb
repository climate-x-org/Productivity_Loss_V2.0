{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Using clix cache dir at: /tmp/dask_\n",
      "INFO:root:Using dask cache dir at: /tmp/dask_\n",
      "/Users/aidanstarr/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/clix/aggregation_interpolation.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "/Users/aidanstarr/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr \n",
    "import clix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidanstarr/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/bokeh/core/property/primitive.py:37: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  bokeh_bool_types += (np.bool8,)\n",
      "INFO:distributed.http.proxy:To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "INFO:distributed.scheduler:State start\n",
      "INFO:distributed.scheduler:  Scheduler at:     tcp://127.0.0.1:64351\n",
      "INFO:distributed.scheduler:  dashboard at:  http://127.0.0.1:8787/status\n",
      "INFO:distributed.scheduler:Registering Worker plugin shuffle\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64354'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64355'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64356'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64357'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64358'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64359'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64360'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64361'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64362'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64363'\n",
      "INFO:distributed.nanny:        Start Nanny at: 'tcp://127.0.0.1:64364'\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64387', name: 7, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64387\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64398\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64386', name: 2, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64386\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64406\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64384', name: 1, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64384\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64410\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64378', name: 3, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64378\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64404\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64383', name: 4, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64383\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64400\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64382', name: 10, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64382\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64401\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64380', name: 0, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64380\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64407\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64381', name: 9, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64381\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64402\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64388', name: 8, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64388\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64403\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64379', name: 6, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64379\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64409\n",
      "INFO:distributed.scheduler:Register worker <WorkerState 'tcp://127.0.0.1:64385', name: 5, status: init, memory: 0, processing: 0>\n",
      "INFO:distributed.scheduler:Starting worker compute stream, tcp://127.0.0.1:64385\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64405\n",
      "INFO:distributed.scheduler:Receive client connection: Client-022d195a-03f4-11f0-ad9d-86c47eeb6459\n",
      "INFO:distributed.core:Starting established connection to tcp://127.0.0.1:64411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x349731fc0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "import s3fs, dask\n",
    "fs = s3fs.S3FileSystem(anon=False, use_listings_cache=False, asynchronous=True)\n",
    "dask.config.set(\n",
    "    {\"s3.multipart_chunksize\": \"50MB\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../data/sample_assets.csv'\n",
    "loss_function = 'HOTHAPS'\n",
    "\n",
    "\n",
    "## read csv  \n",
    "df_in = pd.read_csv(fn)[['asset_id','latitude','longitude','asset_type']]\n",
    "df_in = df_in.drop_duplicates('asset_id').rename(columns={'latitude':'lat','longitude':'lon'})\n",
    "## load asset map \n",
    "asset_map = pd.read_csv('../src/asset_map.csv')\n",
    "\n",
    "## load aircon\n",
    "aircon = xr.open_zarr('s3://hazard-science-data/productivity_loss_v2/aircon/AirCon_SSPs.zarr/') \n",
    "aircon = aircon.sel(SSP='2.0').isel(year=0)['ac_penetration']\n",
    "\n",
    "## load losses\n",
    "ds_dict = {}\n",
    "for scenario in ['ssp126','ssp245','ssp370','ssp585']:\n",
    "    ds_dict[scenario] = {}\n",
    "    for intensity in ['low','moderate','high']:\n",
    "        ds_dict[scenario][intensity] = xr.open_zarr(f\"s3://hazard-science-data/productivity_loss_v2/climate_outputs/projections_corrected/{scenario}/CMIP6-ScenarioMIP_{loss_function}_productivity_loss_{intensity}_{scenario}.zarr.zarr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## output\n",
    "df_out = df_in.copy(deep=True).set_index('asset_id')\n",
    "df_out['work_intensity'] = df_out['asset_type'].map(asset_map.set_index('asset_type').to_dict()['intensity'])\n",
    "df_out=df_out.reset_index()\n",
    "## sample\n",
    "df_out_ssp126 = df_out.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.head().to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th>asset_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2025</th>\n",
       "      <th>1203 - BP INTERNATIONAL LIMITED</th>\n",
       "      <td>51.734730</td>\n",
       "      <td>5.494284</td>\n",
       "      <td>0.034642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19238 - BP INTERNATIONAL LIMITED</th>\n",
       "      <td>51.935600</td>\n",
       "      <td>4.125400</td>\n",
       "      <td>0.022831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21 - DP WORLD LIMITED</th>\n",
       "      <td>51.550000</td>\n",
       "      <td>4.460072</td>\n",
       "      <td>0.022831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5420 - BP INTERNATIONAL LIMITED</th>\n",
       "      <td>51.914513</td>\n",
       "      <td>4.546143</td>\n",
       "      <td>0.022831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72 - ABU DHABI NATIONAL ENERGY COMPANY - P.J.S.C</th>\n",
       "      <td>24.090403</td>\n",
       "      <td>52.652494</td>\n",
       "      <td>12.984390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2100</th>\n",
       "      <th>74 - SIEMENS ENERGY LIMITED</th>\n",
       "      <td>4.904735</td>\n",
       "      <td>114.920770</td>\n",
       "      <td>11.362954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450 - MINOR INTERNATIONAL PUBLIC COMPANY LIMITED</th>\n",
       "      <td>4.905742</td>\n",
       "      <td>114.916441</td>\n",
       "      <td>11.266126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572 - SAMSUNG C AND T CORPORATION</th>\n",
       "      <td>47.874492</td>\n",
       "      <td>106.915138</td>\n",
       "      <td>0.022831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689 - SAMSUNG C AND T CORPORATION</th>\n",
       "      <td>47.920030</td>\n",
       "      <td>106.891174</td>\n",
       "      <td>0.022831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223 - SAMSUNG C AND T CORPORATION</th>\n",
       "      <td>42.842570</td>\n",
       "      <td>74.604920</td>\n",
       "      <td>0.114466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4961 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             lat         lon  \\\n",
       "year asset_id                                                                  \n",
       "2025 1203 - BP INTERNATIONAL LIMITED                   51.734730    5.494284   \n",
       "     19238 - BP INTERNATIONAL LIMITED                  51.935600    4.125400   \n",
       "     21 - DP WORLD LIMITED                             51.550000    4.460072   \n",
       "     5420 - BP INTERNATIONAL LIMITED                   51.914513    4.546143   \n",
       "     72 - ABU DHABI NATIONAL ENERGY COMPANY - P.J.S.C  24.090403   52.652494   \n",
       "...                                                          ...         ...   \n",
       "2100 74 - SIEMENS ENERGY LIMITED                        4.904735  114.920770   \n",
       "     450 - MINOR INTERNATIONAL PUBLIC COMPANY LIMITED   4.905742  114.916441   \n",
       "     1572 - SAMSUNG C AND T CORPORATION                47.874492  106.915138   \n",
       "     1689 - SAMSUNG C AND T CORPORATION                47.920030  106.891174   \n",
       "     1223 - SAMSUNG C AND T CORPORATION                42.842570   74.604920   \n",
       "\n",
       "                                                          median  \n",
       "year asset_id                                                     \n",
       "2025 1203 - BP INTERNATIONAL LIMITED                    0.034642  \n",
       "     19238 - BP INTERNATIONAL LIMITED                   0.022831  \n",
       "     21 - DP WORLD LIMITED                              0.022831  \n",
       "     5420 - BP INTERNATIONAL LIMITED                    0.022831  \n",
       "     72 - ABU DHABI NATIONAL ENERGY COMPANY - P.J.S.C  12.984390  \n",
       "...                                                          ...  \n",
       "2100 74 - SIEMENS ENERGY LIMITED                       11.362954  \n",
       "     450 - MINOR INTERNATIONAL PUBLIC COMPANY LIMITED  11.266126  \n",
       "     1572 - SAMSUNG C AND T CORPORATION                 0.022831  \n",
       "     1689 - SAMSUNG C AND T CORPORATION                 0.022831  \n",
       "     1223 - SAMSUNG C AND T CORPORATION                 0.114466  \n",
       "\n",
       "[4961 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_out = extract_asset_data(ds_dict['ssp126']['high'],df_out_ssp126)\n",
    "ds_out['median'].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/xarray/core/common.py:157\u001b[0m, in \u001b[0;36mAbstractArray.__float__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__float__\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/xarray/core/dataarray.py:814\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03mThe array's data converted to numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03mto this array may be reflected in the DataArray as well.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/xarray/core/variable.py:566\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/xarray/core/variable.py:363\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mit's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03mTODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/dask/array/core.py:1697\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1697\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/dask/base.py:372\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03mThis turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03mdask.compute\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/dask/base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m ds_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mssp126\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39myear\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m newcol \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimum\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximum\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 13\u001b[0m         \u001b[43mdf_out_ssp126\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnewcol\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m ds_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mssp126\u001b[39m\u001b[38;5;124m'\u001b[39m][wi]\u001b[38;5;241m.\u001b[39msel(lat\u001b[38;5;241m=\u001b[39mlat,lon\u001b[38;5;241m=\u001b[39mlon,year\u001b[38;5;241m=\u001b[39myear,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)[newcol]\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/pandas/core/indexing.py:911\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    910\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 911\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/pandas/core/indexing.py:1942\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# align and set the values\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m take_split_path:\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;66;03m# We have to operate column-wise\u001b[39;00m\n\u001b[0;32m-> 1942\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer_split_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1944\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/pandas/core/indexing.py:2035\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2033\u001b[0m     \u001b[38;5;66;03m# scalar value\u001b[39;00m\n\u001b[1;32m   2034\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loc \u001b[38;5;129;01min\u001b[39;00m ilocs:\n\u001b[0;32m-> 2035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/pandas/core/indexing.py:2175\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[0;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mvoid:\n\u001b[1;32m   2166\u001b[0m         \u001b[38;5;66;03m# This means we're expanding, with multiple columns, e.g.\u001b[39;00m\n\u001b[1;32m   2167\u001b[0m         \u001b[38;5;66;03m#     df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]})\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2170\u001b[0m         \u001b[38;5;66;03m# Here, we replace those temporary `np.void` columns with\u001b[39;00m\n\u001b[1;32m   2171\u001b[0m         \u001b[38;5;66;03m# columns of the appropriate dtype, based on `value`.\u001b[39;00m\n\u001b[1;32m   2172\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[:, loc] \u001b[38;5;241m=\u001b[39m construct_1d_array_from_inferred_fill_value(\n\u001b[1;32m   2173\u001b[0m             value, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   2174\u001b[0m         )\n\u001b[0;32m-> 2175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_setitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplane_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/pandas/core/internals/managers.py:1337\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[0;34m(self, loc, idx, value, inplace_only)\u001b[0m\n\u001b[1;32m   1335\u001b[0m     col_mgr\u001b[38;5;241m.\u001b[39msetitem_inplace(idx, value)\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1337\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mcol_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miset(loc, new_mgr\u001b[38;5;241m.\u001b[39m_block\u001b[38;5;241m.\u001b[39mvalues, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_to_warn:\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/pandas/core/internals/managers.py:415\u001b[0m, in \u001b[0;36mBaseBlockManager.setitem\u001b[0;34m(self, indexer, value, warn)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# No need to split if we either set all columns or on a single block\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# manager\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/miniforge3/envs/productivity_loss_v2/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1429\u001b[0m, in \u001b[0;36mBlock.setitem\u001b[0;34m(self, indexer, value, using_cow)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     casted \u001b[38;5;241m=\u001b[39m casted[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1429\u001b[0m     \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m casted\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like(casted):\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "\n",
    "for year in ds_dict['ssp126']['high'].year.values:\n",
    "    for newcol in ['median','minimum','maximum']:\n",
    "        df_out_ssp126[newcol+\"_\"+str(year)] = np.full_like(df_out_ssp126['lat'],np.nan)\n",
    "\n",
    "for i in df_out_ssp126.index:\n",
    "    wi = df_out_ssp126.loc[i,'work_intensity']\n",
    "    lat = df_out_ssp126.loc[i,'lat']\n",
    "    lon = df_out_ssp126.loc[i,'lon']\n",
    "    for year in ds_dict['ssp126']['high'].year.values:\n",
    "        for newcol in ['median','minimum','maximum']:\n",
    "            df_out_ssp126.loc[i,newcol+\"_\"+str(year)] = ds_dict['ssp126'][wi].sel(lat=lat,lon=lon,year=year,method='nearest')[newcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_ssp126 = df_out_ssp126.loc[:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:distributed.scheduler:Detected different `run_spec` for key 'original-open_dataset-minimum-0bf6ad5181abef2fbaefd07a25abdbf3' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \n",
      "Debugging information\n",
      "---------------------\n",
      "old task state: released\n",
      "old run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x380dfb4c0>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "new run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x380dedc40>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "old token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('c961c87336555955', ['eb55f6447623a5c5']),)), ('dict', ())))\n",
      "new token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('c961c87336555955', ['34c96acdcadb1bbb']),)), ('dict', ())))\n",
      "old dependencies: set()\n",
      "new dependencies: set()\n",
      "\n",
      "WARNING:distributed.scheduler:Detected different `run_spec` for key 'original-open_dataset-median-0bf6ad5181abef2fbaefd07a25abdbf3' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \n",
      "Debugging information\n",
      "---------------------\n",
      "old task state: released\n",
      "old run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x377cb0580>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "new run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x377a58340>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "old token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('437dc64c6e09ca03', ['34c96acdcadb1bbb']),)), ('dict', ())))\n",
      "new token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('437dc64c6e09ca03', ['81f7af5b232501a9']),)), ('dict', ())))\n",
      "old dependencies: set()\n",
      "new dependencies: set()\n",
      "\n",
      "WARNING:distributed.scheduler:Detected different `run_spec` for key 'original-open_dataset-median-0bf6ad5181abef2fbaefd07a25abdbf3' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \n",
      "Debugging information\n",
      "---------------------\n",
      "old task state: released\n",
      "old run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x37b97f580>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "new run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x378926a80>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "old token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('437dc64c6e09ca03', ['34c96acdcadb1bbb']),)), ('dict', ())))\n",
      "new token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('437dc64c6e09ca03', ['7c0f81bfbac4235d']),)), ('dict', ())))\n",
      "old dependencies: set()\n",
      "new dependencies: set()\n",
      "\n",
      "WARNING:distributed.scheduler:Detected different `run_spec` for key 'original-open_dataset-minimum-0bf6ad5181abef2fbaefd07a25abdbf3' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \n",
      "Debugging information\n",
      "---------------------\n",
      "old task state: released\n",
      "old run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x377fee240>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "new run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x37b92dc00>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "old token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('c961c87336555955', ['7c0f81bfbac4235d']),)), ('dict', ())))\n",
      "new token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('c961c87336555955', ['34c96acdcadb1bbb']),)), ('dict', ())))\n",
      "old dependencies: set()\n",
      "new dependencies: set()\n",
      "\n",
      "WARNING:distributed.scheduler:Detected different `run_spec` for key 'original-open_dataset-minimum-4c92e86a21144bb46979b31bf8623f9f' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \n",
      "Debugging information\n",
      "---------------------\n",
      "old task state: released\n",
      "old run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x3828e1f00>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "new run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x38285e480>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "old token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('2bf5cd14b174edfa', ['b10c404a145dc98c']),)), ('dict', ())))\n",
      "new token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('2bf5cd14b174edfa', ['34c96acdcadb1bbb']),)), ('dict', ())))\n",
      "old dependencies: set()\n",
      "new dependencies: set()\n",
      "\n",
      "WARNING:distributed.scheduler:Detected different `run_spec` for key 'original-open_dataset-minimum-4c92e86a21144bb46979b31bf8623f9f' between two consecutive calls to `update_graph`. This can cause failures and deadlocks down the line. Please ensure unique key names. If you are using a standard dask collections, consider releasing all the data before resubmitting another computation. More details and help can be found at https://github.com/dask/dask/issues/9888. \n",
      "Debugging information\n",
      "---------------------\n",
      "old task state: released\n",
      "old run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x381115200>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "new run_spec: (<function execute_task at 0x312b13a30>, (ImplicitToExplicitIndexingAdapter(array=CopyOnWriteArray(array=LazilyIndexedArray(array=<xarray.backends.zarr.ZarrArrayWrapper object at 0x38111a680>, key=BasicIndexer((slice(None, None, None), slice(None, None, None), slice(None, None, None)))))),), {})\n",
      "old token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('2bf5cd14b174edfa', ['34c96acdcadb1bbb']),)), ('dict', ())))\n",
      "new token: ('tuple', (('80935a1067ef908b', []), ('tuple', (('2bf5cd14b174edfa', ['87b8166da7ec4841']),)), ('dict', ())))\n",
      "old dependencies: set()\n",
      "new dependencies: set()\n",
      "\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.02s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Scheduler for 917.03s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.87s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Scheduler for 977.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.85s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 977.86s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Nanny for 102.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
      "INFO:distributed.core:Event loop was unresponsive in Scheduler for 102.79s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n"
     ]
    }
   ],
   "source": [
    "years = ds_dict['ssp126']['high'].year.values\n",
    "newcols = ['median', 'minimum', 'maximum']\n",
    "\n",
    "# Create new columns in one operation\n",
    "col_names = [f\"{year}_{stat}\" for year in years for stat in newcols]\n",
    "df_out_ssp126 = df_out_ssp126.assign(**{col: np.nan for col in col_names})\n",
    "\n",
    "# function to extract data from xarray\n",
    "def extract_values(row):\n",
    "    \"\"\"Fetches nearest productivity loss values for given lat, lon, and work intensity.\"\"\"\n",
    "    ds = ds_dict['ssp126'][row['work_intensity']]\n",
    "    result = {f\"{year}_{stat}\": np.round(ds.sel(lat=row['lat'], lon=row['lon'], year=year, method='nearest')[stat].values,4)\n",
    "              for stat in newcols for year in years }\n",
    "    return pd.Series(result)\n",
    "\n",
    "# apply function across all rows\n",
    "df_out_ssp126[col_names] = df_out_ssp126.apply(extract_values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_asset_data(ds, asset_df, lat_name=\"lat\", lon_name=\"lon\", time_name=\"year\"):\n",
    "    \"\"\"\n",
    "    Extracts values from a gridded xarray dataset at specific asset locations.\n",
    "    \n",
    "    Parameters:\n",
    "        ds (xr.Dataset): Gridded dataset with dimensions (time, lat, lon).\n",
    "        asset_df (pd.DataFrame): DataFrame with 'asset_id', 'lat', and 'lon'.\n",
    "        lat_name (str): Name of latitude dimension in dataset (default: \"lat\").\n",
    "        lon_name (str): Name of longitude dimension in dataset (default: \"lon\").\n",
    "        time_name (str): Name of time dimension in dataset (default: \"year\").\n",
    "    \n",
    "    Returns:\n",
    "        xr.Dataset: Dataset with dimensions (asset_id, year) containing extracted values.\n",
    "    \"\"\"\n",
    "    # Convert asset locations into xarray DataArray\n",
    "    asset_lats = xr.DataArray(asset_df[\"lat\"], dims=\"asset_id\", coords={\"asset_id\": asset_df[\"asset_id\"]})\n",
    "    asset_lons = xr.DataArray(asset_df[\"lon\"], dims=\"asset_id\", coords={\"asset_id\": asset_df[\"asset_id\"]})\n",
    "\n",
    "    # Interpolate dataset values at asset locations\n",
    "    extracted_data = ds.interp({lat_name: asset_lats, lon_name: asset_lons}, method=\"linear\")\n",
    "\n",
    "    # Reshape dataset to include asset_id, lat, lon\n",
    "    extracted_data = extracted_data.assign_coords(\n",
    "        lat=(\"asset_id\", asset_df[\"lat\"]),\n",
    "        lon=(\"asset_id\", asset_df[\"lon\"])\n",
    "    )\n",
    "\n",
    "    return extracted_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "productivity_loss_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
