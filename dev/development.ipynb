{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr \n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def workability_functions(wbgt, intensity=\"moderate\", loss_function=\"HOTHAPS\"):\n",
    "    \"\"\"\n",
    "    Compute workability loss based on WBGT (Wet-Bulb Globe Temperature) and work intensity.\n",
    "\n",
    "    Parameters:\n",
    "        wbgt (float or np.ndarray): Wet-Bulb Globe Temperature in ºC.\n",
    "        intensity (str, optional): Work intensity level. Options: \"low\", \"moderate\", \"high\".\n",
    "                                   - \"low\" = 200W (office work)\n",
    "                                   - \"moderate\" = 300W (manufacturing)\n",
    "                                   - \"high\" = 400W (construction/agriculture)\n",
    "        loss_function (str, optional): Method used to compute loss. Currently supports \"HOTHAPS\", \"ISO\", and \"NIOSH\"\n",
    "\n",
    "    Returns:\n",
    "        float or np.ndarray: Workability loss as a fraction (0 to 1).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unrecognized `loss_function` or `intensity` is provided.\n",
    "    \"\"\"\n",
    "\n",
    "    if intensity not in [\"low\", \"moderate\", \"high\"]:\n",
    "        raise ValueError(\n",
    "            f\"Invalid intensity: {intensity}. Must be one of ['low','moderate','high'].\"\n",
    "        )\n",
    "\n",
    "    M_dict = {\"low\": 200, \"moderate\": 300, \"high\": 400}\n",
    "\n",
    "    if loss_function.lower() == \"hothaps\":\n",
    "        coefs = {\n",
    "            \"low\": [34.64, 22.72],\n",
    "            \"moderate\": [32.93, 17.81],\n",
    "            \"high\": [30.94, 16.64],\n",
    "        }\n",
    "        a1, a2 = coefs[intensity]\n",
    "        workability = 0.1 + (0.9 / (1 + (wbgt / a1) ** a2))\n",
    "\n",
    "    elif loss_function.lower() == \"iso\":\n",
    "        WBGT_lim = 34.9 - (M_dict[intensity] / 46)\n",
    "        # Reference WBGT limit for resting metabolic rate (M_rest = 117W)\n",
    "        WBGT_lim_rest = 34.9 - (117 / 46)\n",
    "        # Compute workability using Equation (3)\n",
    "        workability = np.maximum(\n",
    "            0, np.minimum(1, (WBGT_lim_rest - wbgt) / (WBGT_lim_rest - WBGT_lim))\n",
    "        )\n",
    "\n",
    "    elif loss_function.lower() == \"niosh\":\n",
    "        WBGT_lim = 56.7 - (11.5 * np.log10(M_dict[intensity]))\n",
    "        # Reference WBGT limit for resting metabolic rate (M_rest = 117W)\n",
    "        WBGT_lim_rest = 56.7 - (11.5 * np.log10(117))\n",
    "        # Compute workability using Equation (3)\n",
    "        workability = np.maximum(\n",
    "            0, np.minimum(1, (WBGT_lim_rest - wbgt) / (WBGT_lim_rest - WBGT_lim))\n",
    "        )\n",
    "    else:\n",
    "        ValueError(\"Loss function needs be one of ISO, NIOSH, HOTHAPS\")\n",
    "\n",
    "    return workability\n",
    "\n",
    "\n",
    "def plot_workability_functions():\n",
    "    with plt.style.context(\"seaborn-v0_8-whitegrid\"):\n",
    "        x = np.arange(0, 50, 1)\n",
    "        standard = \"HOTHAPS\"\n",
    "        for M, color in zip(\n",
    "            [\"low\", \"moderate\", \"high\"],\n",
    "            [\"xkcd:navy\",\"xkcd:goldenrod\",\"xkcd:ruby\"],\n",
    "        ):\n",
    "            ls = {\"ISO\": \":\", \"NIOSH\": \"-\", \"HOTHAPS\": \"-\"}\n",
    "            lw = {\"ISO\": 2.3, \"NIOSH\": 2, \"HOTHAPS\": 2}\n",
    "            plt.plot(\n",
    "                x,\n",
    "                workability_functions(x, M, standard),\n",
    "                label=f\"{M} intensity\",\n",
    "                linestyle=ls[standard],\n",
    "                linewidth=lw[standard],\n",
    "                color=color,\n",
    "            )\n",
    "\n",
    "        plt.xlim(20, 45)\n",
    "        plt.legend(ncols=1, bbox_to_anchor=(0.52, 0.7),fontsize=13,frameon=False)\n",
    "        plt.ylabel(\"Workability fraction\",fontsize=14)\n",
    "        plt.xlabel(\"WBGT [ºC]\",fontsize=14)\n",
    "        plt.gca().tick_params(axis='both', which='major',direction='in', labelsize=12,length=3)\n",
    "        plt.grid(False)\n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_workability_functions()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_zarr('s3://climate-zarr/CMIP6/ScenarioMIP/bias_corrected/DQM/CMIP6-ScenarioMIP_WBGTmean_ssp585.zarr/')\n",
    "\n",
    "bbox = {\n",
    "    \"lon_min\": -10,\n",
    "    \"lon_max\": 20,\n",
    "    \"lat_min\": 35,\n",
    "    \"lat_max\": 60,\n",
    "}\n",
    "plot_3d_temperature(ds.isel(time=0).sel(lat=slice(bbox['lat_max'],bbox['lat_min']),lon=slice(bbox['lon_min'],bbox['lon_max'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.feature as cfeature\n",
    "from shapely.geometry import box\n",
    "from shapely.ops import unary_union\n",
    "from shapely.vectorized import contains\n",
    "import cmocean as cmo\n",
    "\n",
    "def create_land_mask(lon_grid, lat_grid, bbox=None):\n",
    "    \"\"\"\n",
    "    Create a land mask using Natural Earth land geometries.\n",
    "    \n",
    "    Parameters:\n",
    "      lon_grid, lat_grid: 2D numpy arrays representing the meshgrid of longitudes and latitudes.\n",
    "      bbox: dict or None\n",
    "          Optional bounding box defined as a dictionary with keys:\n",
    "          'lon_min', 'lon_max', 'lat_min', 'lat_max'. If provided, the land\n",
    "          geometry is intersected with this box.\n",
    "    \n",
    "    Returns:\n",
    "      mask: Boolean numpy array with the same shape as lon_grid.\n",
    "            True indicates a land point; False indicates ocean.\n",
    "    \"\"\"\n",
    "    # Get Natural Earth land feature (110m resolution)\n",
    "    land_feature = cfeature.NaturalEarthFeature('physical', 'land', '110m')\n",
    "    land_geoms = list(land_feature.geometries())\n",
    "    land_union = unary_union(land_geoms)\n",
    "    \n",
    "    if bbox is not None:\n",
    "        # Create a bounding box polygon and intersect with the land geometry.\n",
    "        bbox_poly = box(bbox['lon_min'], bbox['lat_min'], bbox['lon_max'], bbox['lat_max'])\n",
    "        land_union = land_union.intersection(bbox_poly)\n",
    "    \n",
    "    # Create the mask using shapely.vectorized.contains\n",
    "    mask = contains(land_union, lon_grid, lat_grid)\n",
    "    return mask\n",
    "\n",
    "def plot_3d_temperature(ds, z_spacing=20):\n",
    "    \"\"\"\n",
    "    Create a 3D plot with stacked layers for each member_id and mask out ocean regions.\n",
    "    \n",
    "    Parameters:\n",
    "      ds : xarray.Dataset\n",
    "          Dataset with coordinates 'lat', 'lon', and 'member_id'.\n",
    "          The variable 'temperature' is assumed to have dimensions \n",
    "          ('lat', 'lon', 'member_id').\n",
    "      z_spacing : float, optional\n",
    "          Vertical offset between layers (default is 10).\n",
    "    \"\"\"\n",
    "    # Extract coordinate values.\n",
    "    lats = ds['lat'].values\n",
    "    lons = ds['lon'].values\n",
    "    member_ids = ds['member_id'].values\n",
    "\n",
    "    # Create a meshgrid for lat/lon.\n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Normalize temperature for the colormap.\n",
    "    temp_min = float(ds['WBGTmean'].min().values)+8\n",
    "    temp_max = float(ds['WBGTmean'].max().values)\n",
    "    norm = plt.Normalize(temp_min, temp_max)\n",
    "    cmap = plt.get_cmap('cmo.thermal')\n",
    "\n",
    "    # Define a bounding box for Western Europe.\n",
    "    western_europe_bbox = {\n",
    "        \"lon_min\": -10,\n",
    "        \"lon_max\": 20,\n",
    "        \"lat_min\": 35,\n",
    "        \"lat_max\": 60,\n",
    "    }\n",
    "\n",
    "    # Create a land mask for the grid.\n",
    "    land_mask = create_land_mask(lon_grid, lat_grid, bbox=western_europe_bbox)\n",
    "\n",
    "    # Loop over each member_id and plot the corresponding temperature surface.\n",
    "    for idx, member in enumerate(member_ids):\n",
    "        # Extract temperature for the current member_id (2D: [lat, lon]).\n",
    "        temp = ds['WBGTmean'].sel(member_id=member).values\n",
    "\n",
    "        # Set a vertical offset for this member layer.\n",
    "        z_offset = idx * z_spacing\n",
    "        z_layer = np.full_like(lon_grid, z_offset)\n",
    "        \n",
    "        # Compute facecolors based on temperature.\n",
    "        facecolors = cmap(norm(temp))\n",
    "        # Mask out ocean points: set alpha channel to 0 where land_mask is False.\n",
    "        facecolors[~land_mask, 3] = 0\n",
    "\n",
    "        # Plot the surface with the masked facecolors.\n",
    "        ax.plot_surface(\n",
    "            lon_grid, lat_grid, z_layer,\n",
    "            facecolors=facecolors,\n",
    "            rstride=1, cstride=1, antialiased=True, shade=False\n",
    "        )\n",
    "    \n",
    "    # Set axis labels.\n",
    "    ax.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = xr.open_zarr('s3://hazard-science-data/productivity_loss_v2/aircon/AirCon_SSPs.zarr/')\n",
    "dsss = dss.sel(SSP=2,year=2020)\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "with plt.style.context(\"seaborn-v0_8-whitegrid\"):\n",
    "    f = plt.figure(figsize=(7,5))\n",
    "    ax = f.add_subplot(111,projection=ccrs.Robinson())\n",
    "    fg = dsss['ac_penetration'].plot(ax=ax,transform=ccrs.PlateCarree(),cmap='bone_r',cbar_kwargs={'orientation':'horizontal','shrink':0.4,'label':'AC ownership fraction','pad':0.05},vmin=0,vmax=1)\n",
    "    plt.title('')\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_ticks([0, 1])\n",
    "    cbar.set_ticklabels(['Low\\nProbability','High\\nProbability'],fontsize=12)\n",
    "    ax.coastlines(lw=0.5)\n",
    "    cbar.set_label(label='AC ownership', size=13)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('cmo.thermal')\n",
    "cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.get_cmap('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time=0).sel(lat=slice(bbox['lat_min'],bbox['lat_max']),lon=slice(bbox['lon_min'],bbox['lon_max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "import s3fs, dask\n",
    "fs = s3fs.S3FileSystem(anon=False, use_listings_cache=False, asynchronous=True)\n",
    "dask.config.set(\n",
    "    {\"s3.multipart_chunksize\": \"50MB\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(fn,encoding='unicode_escape')[['Asset ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '../data/siemens-energy-ag.csv'\n",
    "loss_function = 'HOTHAPS'\n",
    "\n",
    "\n",
    "## read csv  \n",
    "df_in = pd.read_csv(fn,encoding='unicode_escape')[['Asset ID','Latitude','Longitude','Parent Name','Premise Type','Country']]\n",
    "df_in = df_in.drop_duplicates('Asset ID').rename(columns={'Latitude':'lat','Longitude':'lon','Country':'country'})\n",
    "## load asset map \n",
    "asset_map = pd.read_csv('../src/asset_map.csv')\n",
    "\n",
    "## load aircon\n",
    "aircon = xr.open_zarr('s3://hazard-science-data/productivity_loss_v2/aircon/AirCon_SSPs.zarr/') \n",
    "aircon = aircon.sel(SSP='2.0').isel(year=0)['ac_penetration'].chunk(dict(lon=-1)).interpolate_na(method='nearest',dim='lon',limit=3)\n",
    "\n",
    "\n",
    "## load losses\n",
    "ds_dict = {}\n",
    "ds_2020 = {}\n",
    "for intensity in ['low','moderate','high']:\n",
    "    ds_2020[intensity] = xr.open_zarr(f's3://hazard-science-data/productivity_loss_v2/climate_outputs/observations/ERA5_{loss_function}_productivity_loss_{intensity}.zarr.zarr/')\n",
    "for scenario in ['ssp126','ssp245','ssp370','ssp585']:\n",
    "    ds_dict[scenario] = {}\n",
    "    for intensity in ['low','moderate','high']:\n",
    "        ds_dict[scenario][intensity] = xr.open_zarr(f\"s3://hazard-science-data/productivity_loss_v2/climate_outputs/projections_corrected/{scenario}/CMIP6-ScenarioMIP_{loss_function}_productivity_loss_{intensity}_{scenario}.zarr.zarr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scenarios = ['ssp585']\n",
    "\n",
    "# List to accumulate all rows (one per asset_id, year, scenario)\n",
    "rows_list = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    # Create a temporary copy of your asset data and map work_intensity\n",
    "    df_temp = df_in.loc[:40, :].copy(deep=True)\n",
    "    df_temp['work_intensity'] = df_temp['asset_type'].map(asset_map.set_index('asset_type')['intensity'])\n",
    "    \n",
    "    # Get the years available for this scenario.\n",
    "    # Here, we assume the years are the same for every asset in a scenario.\n",
    "    years = np.concat([[2020],ds_dict[scenario]['high'].year.values])\n",
    "    stats = ['median', 'minimum', 'maximum']\n",
    "    \n",
    "    # Process each asset row\n",
    "    for _, row in df_temp.iterrows():\n",
    "        # Get the dataset corresponding to the asset's work intensity in the current scenario\n",
    "        ds = ds_dict[scenario][row['work_intensity']]\n",
    "        ds_2020_local = ds_2020[row['work_intensity']].sel(lat=row['lat'], lon=row['lon'], method='nearest')\n",
    "\n",
    "        # For each year, extract the values for all stats and build a new row\n",
    "        for year in years:\n",
    "            row_dict = row.to_dict()  # Copy the asset info (like asset_id, lat, lon, etc.)\n",
    "            row_dict['scenario'] = scenario\n",
    "            \n",
    "            if year == 2020:\n",
    "                row_dict['year'] = 2020\n",
    "                for stat in stats:\n",
    "                    value = np.round(ds_2020_local['median'].values,2)\n",
    "                    row_dict[stat] = value\n",
    "\n",
    "            else:\n",
    "                row_dict['year'] = year\n",
    "                # Get each statistic value from the dataset (using nearest neighbor)\n",
    "                for stat in stats:\n",
    "                    value = np.round(ds.sel(lat=row['lat'], lon=row['lon'], year=year, method='nearest')[stat].values, 2)\n",
    "                    row_dict[stat] = value\n",
    "                # Append the new row to our list\n",
    "            rows_list.append(row_dict)\n",
    "\n",
    "# Create the final long DataFrame\n",
    "df_long = pd.DataFrame(rows_list)\n",
    "df_long.to_csv(f\"../output_csvs/unscaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a deep copy of df_long so that the original remains unchanged\n",
    "df_long_scaled = df_long.copy(deep=True)\n",
    "\n",
    "# Create a new column for AC_penetration and initialize with NaN\n",
    "df_long_scaled['AC_penetration'] = np.nan\n",
    "\n",
    "# Iterate over each row to compute AC penetration and adjust the values\n",
    "for i in df_long_scaled.index:\n",
    "    lat = df_long_scaled.loc[i, 'lat']\n",
    "    lon = df_long_scaled.loc[i, 'lon']\n",
    "    # Compute AC_penetration using the nearest neighbor from the aircon dataset\n",
    "    ac_val = np.round(aircon.sel(lat=lat, lon=lon, method='nearest').values, 2)\n",
    "    df_long_scaled.loc[i, 'AC_penetration'] = ac_val\n",
    "\n",
    "    # If a valid AC penetration value is found, scale the statistics accordingly\n",
    "    if not pd.isna(ac_val):\n",
    "        # Scale 'median', 'minimum', and 'maximum' by multiplying with (1 - AC_penetration)\n",
    "        df_long_scaled.loc[i, ['median', 'minimum', 'maximum']] *= (1 - ac_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import random\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define scenarios, e.g., ['ssp126', 'ssp585']\n",
    "scenarios = ['ssp126','ssp585']\n",
    "\n",
    "# Select 25 random asset_ids from the entire long-format DataFrame\n",
    "sample_ids = random.sample(list(df_long_scaled['asset_id'].unique()), 25)\n",
    "\n",
    "# Create a subplot with one row per scenario\n",
    "f, ax = plt.subplots(len(scenarios), 1, figsize=(10, 3 * len(scenarios)), sharex=True)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    # Filter for the current scenario, the sampled asset_ids, and only the years 2025 and 2100.\n",
    "    data = df_long_scaled[\n",
    "        (df_long_scaled['scenario'] == scenario) &\n",
    "        (df_long_scaled['asset_id'].isin(sample_ids)) &\n",
    "        (df_long_scaled['year'].isin([2025, 2100]))\n",
    "    ].copy()\n",
    "    \n",
    "    # Pivot so each asset_id is one row, and columns become \"2025_median\", \"2025_minimum\", etc.\n",
    "    data_pivot = data.pivot(index='asset_id', columns='year', values=['median', 'minimum', 'maximum'])\n",
    "    # Flatten the MultiIndex columns to single strings.\n",
    "    data_pivot.columns = [f\"{year}_{stat}\" for stat, year in data_pivot.columns]\n",
    "    data_pivot = data_pivot.reset_index()\n",
    "\n",
    "    # Define x-axis positions for each asset.\n",
    "    x = np.arange(len(data_pivot))\n",
    "\n",
    "    # Define error bars for 2025 and 2100.\n",
    "    err_2025 = [np.abs(data_pivot[\"2025_median\"] - data_pivot[\"2025_minimum\"]),\n",
    "                np.abs(data_pivot[\"2025_maximum\"] - data_pivot[\"2025_median\"])]\n",
    "    err_2100 = [np.abs(data_pivot[\"2100_median\"] - data_pivot[\"2100_minimum\"]),\n",
    "                np.abs(data_pivot[\"2100_maximum\"] - data_pivot[\"2100_median\"])]\n",
    "\n",
    "    bar_width = 0.4\n",
    "    # Select the appropriate axis handle.\n",
    "    a = ax[i] if len(scenarios) > 1 else ax\n",
    "\n",
    "    # Plot bars with error bars for both years.\n",
    "    a.bar(x - bar_width / 2, data_pivot[\"2025_median\"], yerr=err_2025, width=bar_width,\n",
    "          capsize=5, label=\"2020\", color=\"xkcd:navy\")\n",
    "    a.bar(x + bar_width / 2, data_pivot[\"2100_median\"], yerr=err_2100, width=bar_width,\n",
    "          capsize=5, label=\"2100\", color=\"xkcd:ruby\")\n",
    "\n",
    "    a.set_xticks(x)\n",
    "    a.set_xticklabels(data_pivot['asset_id'], rotation=45,fontsize=6,ha = 'right')\n",
    "    a.set_ylim(0, 50)\n",
    "    # a.set_yscale('log')\n",
    "    a.set_ylabel('Productivity Loss (%)')\n",
    "    a.legend()\n",
    "    a.set_title(scenario)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define scenarios, e.g., ['ssp126', 'ssp585']\n",
    "scenarios = ['ssp126','ssp585']\n",
    "\n",
    "# Select 25 random asset_ids from the entire long-format DataFrame\n",
    "sample_ids = random.sample(list(df_long_scaled['asset_id'].unique()), 25)\n",
    "\n",
    "# Create a subplot with one row per scenario\n",
    "f, ax = plt.subplots(len(scenarios), 1, figsize=(10, 3 * len(scenarios)), sharex=True)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    # Filter for the current scenario, the sampled asset_ids, and only the years 2025 and 2100.\n",
    "    data = df_long_scaled[\n",
    "        (df_long_scaled['scenario'] == scenario) &\n",
    "        (df_long_scaled['asset_id'].isin(sample_ids)) &\n",
    "        (df_long_scaled['year'].isin([2025, 2100]))\n",
    "    ].copy()\n",
    "    \n",
    "    # Pivot so each asset_id is one row, and columns become \"2025_median\", \"2025_minimum\", etc.\n",
    "    data_pivot = data.pivot(index='asset_id', columns='year', values=['median', 'minimum', 'maximum'])\n",
    "    # Flatten the MultiIndex columns to single strings.\n",
    "    data_pivot.columns = [f\"{year}_{stat}\" for stat, year in data_pivot.columns]\n",
    "    data_pivot = data_pivot.reset_index()\n",
    "\n",
    "    # Define x-axis positions for each asset.\n",
    "    x = np.arange(len(data_pivot))\n",
    "\n",
    "    # Define error bars for 2025 and 2100.\n",
    "    err_2025 = [np.abs(data_pivot[\"2025_median\"] - data_pivot[\"2025_minimum\"]),\n",
    "                np.abs(data_pivot[\"2025_maximum\"] - data_pivot[\"2025_median\"])]\n",
    "    err_2100 = [np.abs(data_pivot[\"2100_median\"] - data_pivot[\"2100_minimum\"]),\n",
    "                np.abs(data_pivot[\"2100_maximum\"] - data_pivot[\"2100_median\"])]\n",
    "\n",
    "    bar_width = 0.4\n",
    "    # Select the appropriate axis handle.\n",
    "    a = ax[i] if len(scenarios) > 1 else ax\n",
    "\n",
    "    # Plot bars with error bars for both years.\n",
    "    a.bar(x - bar_width / 2, data_pivot[\"2025_median\"], yerr=err_2025, width=bar_width,\n",
    "          capsize=5, label=\"2020\", color=\"xkcd:grey\")\n",
    "    a.bar(x + bar_width / 2, data_pivot[\"2100_median\"], yerr=err_2100, width=bar_width,\n",
    "          capsize=5, label=\"2100\", color=\"xkcd:black\")\n",
    "\n",
    "    a.set_xticks(x)\n",
    "    a.set_xticklabels(data_pivot['asset_id'], rotation=45,fontsize=6,ha = 'right')\n",
    "    a.set_ylim(0.01, 100)\n",
    "    a.set_yscale('log')\n",
    "    a.set_ylabel('Productivity Loss (%)')\n",
    "    a.legend()\n",
    "    a.set_title(scenario)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = random.sample(list(df_long_scaled['asset_id'].unique()),10)\n",
    "f,ax = plt.subplots(len(ids),1,figsize=(7,2*len(ids)),sharex=True)\n",
    "\n",
    "colors = {\n",
    "    'ssp126':'xkcd:navy',\n",
    "    'ssp245':'xkcd:teal',\n",
    "    'ssp370':'xkcd:orange',\n",
    "    'ssp585':'xkcd:ruby'\n",
    "}\n",
    "\n",
    "for j,id in enumerate(ids):\n",
    "    if len(scenarios)>1:\n",
    "        a = ax[j]\n",
    "    else:\n",
    "        a = ax\n",
    "    \n",
    "\n",
    "    for i, scenario in enumerate(scenarios):\n",
    "        y = df_long_scaled[(df_long_scaled['asset_id']==id)&(df_long_scaled['scenario']==scenario)]['median']\n",
    "        yhigh = df_long_scaled[(df_long_scaled['asset_id']==id)&(df_long_scaled['scenario']==scenario)]['maximum']\n",
    "        ylow = df_long_scaled[(df_long_scaled['asset_id']==id)&(df_long_scaled['scenario']==scenario)]['minimum']\n",
    "        x = [2020,2025,2030,2035,2040,2045,2050,2060,2070,2080,2090,2100]\n",
    "\n",
    "        a.fill_between(x,ylow,yhigh,alpha=0.2,color=colors[scenario])\n",
    "        a.plot(x,y,color=colors[scenario],label=scenario)\n",
    "    a.set_ylim(0,np.max(yhigh)+1)\n",
    "    plt.xlim(2020,2100)\n",
    "    # a.set_yscale('log')\n",
    "    a.set_ylabel('Productivity Loss (%)')\n",
    "    a.legend()\n",
    "    a.set_title(id + ' (' + df_long_scaled[(df_long_scaled['asset_id']==id)]['country'].unique() + ')',fontsize=9)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "productivity_loss_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
